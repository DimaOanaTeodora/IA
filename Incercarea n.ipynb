{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, absolute_import\n",
    "import tensorflow.compat.v1 as tf\n",
    "# import tensorflow.compat.v1 as tf\n",
    "# tf.disable_v2_behavior() \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# citire imagini de antrenare plus etichete de antrenare\n",
    "f = open (\"train.txt\")\n",
    "train_images = []\n",
    "train_labels = []\n",
    "for line in enumerate(open(\"train.txt\", \"r\")):\n",
    "    v = line[1].split(',')\n",
    "    image_name = 'train/'+v[0]\n",
    "    label = int(v[1])\n",
    "    train_labels.append(label)\n",
    "    image = plt.imread(image_name)\n",
    "    train_images.append(image)\n",
    "    \n",
    "train_images = np.expand_dims(train_images, axis=3)\n",
    "train_images = np.array(train_images)\n",
    "# train_labels = np.array(train_labels)\n",
    "\n",
    "\n",
    "# citire imagini de testare \n",
    "f = open (\"test.txt\")\n",
    "test_images = []\n",
    "name_test_images =[]\n",
    "for line in enumerate(open(\"test.txt\", \"r\")):\n",
    "    image_name = 'test/'+ line[1][:len(line[1])-1]\n",
    "    name_test_images.append(line[1][:len(line[1])-1])\n",
    "    image = plt.imread(image_name)\n",
    "    test_images.append(image)\n",
    "    \n",
    "test_images = np.expand_dims(test_images, axis=3)\n",
    "\n",
    "# citire imagini de validare plus etichete de validare\n",
    "f = open (\"validation.txt\")\n",
    "validation_images = []\n",
    "validation_labels = []\n",
    "for line in enumerate(open(\"validation.txt\", \"r\")):\n",
    "    v = line[1].split(',')\n",
    "    image_name = 'validation/'+v[0]\n",
    "    label = int(v[1])\n",
    "    label = v[1]\n",
    "    validation_labels.append(label)\n",
    "    image = plt.imread(image_name)\n",
    "    validation_images.append(image)\n",
    "\n",
    "validation_images = np.expand_dims(validation_images, axis=3)\n",
    "validation_labels = np.array(validation_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 9 #9 categorii notate de la 0 la 8\n",
    "\n",
    "X_train=train_images\n",
    "X_test=validation_images\n",
    "test_images=test_images\n",
    "\n",
    "Y_test=np_utils.to_categorical(validation_labels, n_classes) \n",
    "Y_train=np_utils.to_categorical(train_labels, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras.applications import VGG16\n",
    "\n",
    "# include top should be False to remove the softmax layer\n",
    "pretrained_model = VGG16(include_top=False, weights='imagenet')\n",
    "# pretrained_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# create a new generator\n",
    "imagegen = ImageDataGenerator()\n",
    "\n",
    "# load train data\n",
    "# train = imagegen.flow_from_directory(\"train/\", class_mode=\"categorical\", shuffle=False, batch_size=128, target_size=(224, 224))\n",
    "train = imagegen.fit(train_images, train_labels)\n",
    "# load val data\n",
    "# val = imagegen.flow_from_directory(\"validation/\", class_mode=\"categorical\", shuffle=False, batch_size=128, target_size=(224, 224))\n",
    "\n",
    "# train = imagegen.train(train_images, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The two structures don't have the same nested structure.\n\nFirst structure: type=NoneTensorSpec str=NoneTensorSpec()\n\nSecond structure: type=Tensor str=Tensor(\"input_4:0\", shape=(?, ?, ?, 3), dtype=float32)\n\nMore specifically: Substructure \"type=NoneTensorSpec str=NoneTensorSpec()\" is a sequence, while substructure \"type=Tensor str=Tensor(\"input_4:0\", shape=(?, ?, ?, 3), dtype=float32)\" is not\nEntire first structure:\n.\nEntire second structure:\n.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36massert_same_structure\u001b[1;34m(nest1, nest2, check_types, expand_composites)\u001b[0m\n\u001b[0;32m    402\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 403\u001b[1;33m     _pywrap_utils.AssertSameStructure(nest1, nest2, check_types,\n\u001b[0m\u001b[0;32m    404\u001b[0m                                       expand_composites)\n",
      "\u001b[1;31mValueError\u001b[0m: The two structures don't have the same nested structure.\n\nFirst structure: type=NoneTensorSpec str=NoneTensorSpec()\n\nSecond structure: type=Tensor str=Tensor(\"input_4:0\", shape=(?, ?, ?, 3), dtype=float32)\n\nMore specifically: Substructure \"type=NoneTensorSpec str=NoneTensorSpec()\" is a sequence, while substructure \"type=Tensor str=Tensor(\"input_4:0\", shape=(?, ?, ?, 3), dtype=float32)\" is not",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-0213705a0a2a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# extract train and val features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mvgg_features_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpretrained_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;31m# vgg_features_val = pretrained_model.predict(test_images)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_v1.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    980\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    981\u001b[0m     \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_select_training_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 982\u001b[1;33m     return func.predict(\n\u001b[0m\u001b[0;32m    983\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays_v1.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, model, x, batch_size, verbose, steps, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    702\u001b[0m               **kwargs):\n\u001b[0;32m    703\u001b[0m     \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_or_infer_batch_size\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 704\u001b[1;33m     x, _, _ = model._standardize_user_data(\n\u001b[0m\u001b[0;32m    705\u001b[0m         x, check_steps=True, steps_name='steps', steps=steps)\n\u001b[0;32m    706\u001b[0m     return predict_loop(\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_v1.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[0;32m   2328\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2329\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2330\u001b[1;33m     return self._standardize_tensors(\n\u001b[0m\u001b[0;32m   2331\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2332\u001b[0m         \u001b[0mrun_eagerly\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrun_eagerly\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_v1.py\u001b[0m in \u001b[0;36m_standardize_tensors\u001b[1;34m(self, x, y, sample_weight, run_eagerly, dict_inputs, is_dataset, class_weight, batch_size)\u001b[0m\n\u001b[0;32m   2398\u001b[0m     \u001b[0mflat_expected_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpand_composites\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2399\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mflat_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflat_expected_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2400\u001b[1;33m       \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massert_same_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpand_composites\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2401\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2402\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36massert_same_structure\u001b[1;34m(nest1, nest2, check_types, expand_composites)\u001b[0m\n\u001b[0;32m    406\u001b[0m     \u001b[0mstr1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0m_DOT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnest1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    407\u001b[0m     \u001b[0mstr2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0m_DOT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnest2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 408\u001b[1;33m     raise type(e)(\"%s\\n\"\n\u001b[0m\u001b[0;32m    409\u001b[0m                   \u001b[1;34m\"Entire first structure:\\n%s\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    410\u001b[0m                   \u001b[1;34m\"Entire second structure:\\n%s\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The two structures don't have the same nested structure.\n\nFirst structure: type=NoneTensorSpec str=NoneTensorSpec()\n\nSecond structure: type=Tensor str=Tensor(\"input_4:0\", shape=(?, ?, ?, 3), dtype=float32)\n\nMore specifically: Substructure \"type=NoneTensorSpec str=NoneTensorSpec()\" is a sequence, while substructure \"type=Tensor str=Tensor(\"input_4:0\", shape=(?, ?, ?, 3), dtype=float32)\" is not\nEntire first structure:\n.\nEntire second structure:\n."
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "# extract train and val features\n",
    "vgg_features_train = pretrained_model.predict(train, verbose=1, steps=1)\n",
    "# vgg_features_val = pretrained_model.predict(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization\n",
    "model2 = Sequential()\n",
    "model2.add(Flatten(input_shape=(7,7,512)))\n",
    "model2.add(Dense(100, activation='relu'))\n",
    "model2.add(Dropout(0.5))\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# compile the model\n",
    "model2.compile(optimizer='adam', metrics=['accuracy'], loss='categorical_crossentropy')\n",
    "\n",
    "model2.summary()\n",
    "\n",
    "# train model using features generated from VGG16 model\n",
    "model2.fit(vgg_features_train, train_target, epochs=50, batch_size=128, validation_data=(vgg_features_val, val_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
